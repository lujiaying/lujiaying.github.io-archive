title: 逻辑回归模型--分类标签的概率分布
date: 2016-04-17 23:32:05
tags: [机器学习, 逻辑回归]
---
逻辑回归(Logistic Regression, LR)模型其实仅在线性回归的基础上，套用了一个sigmod函数，但也就由于这个sigmod函数，使得逻辑回归模型成为了机器学习领域一颗耀眼的明星，更是计算广告学的核心。
关于LR模型的数学原理和参数求解方法，已经有很多优秀的文章介绍过，在此我不再赘述。今天给大家介绍的主题是逻辑回归模型中分类标签的概率分布。
<!-- more -->

> 基于经典统计学学派的观点，逻辑回归的分类标签是基于样本特征通过二项分布产生的，分类器要做的实际上就是估计这个分布。

为什么说逻辑回归的分类标签是服从二项分布的呢？
请先看逻辑回归的hypothesis（假设）函数:
![Hypothesis function](http://7xkdra.com1.z0.glb.clouddn.com/image%2Fblog%2FLR_hypothesis_function.png)

LR模型是这样工作的：对待分类样本的特征向量x，把x带入h函数，算得hθ(x)。若hθ(x)大于预设门限（一般是0.5），则判定待分类样本属于类别1，否则属于类别0。
简单来说，逻辑回归模型希望找到一个合适的θ，使得h函数在预测集上能有足够好的表现。在此，我们假设预测集和训练集满足相同的概率分布。

那么，预测集和训练集应该满足怎样的概率分布呢？先贤们已经得出结论，**二项分布**。
请回想二项分布的性质：
1）重复进行n次随机试验，n次试验相互独立，且事件发生与否的概率在每一次独立试验中都保持不变。
2）每次试验仅有两个可能结果，且两种结果互斥。
逻辑回归的h函数正是基于 “样本分类标签满足二项分布”的假设而推导出来的。

### 性质1的说明
针对性质1，若样本分类标签不满足“n次试验相互独立，且事件发生与否的概率在每一次独立试验中都保持不变”，则对应到h函数，针对相同的自变量x，因变量h(x)会有不同的输出值，进而给出不同的分类标签y，这与我们对hypothesis 函数的定义相矛盾。
上面这段话可能有点费解，让我用一个例子解释：我训练了一个女友心情分类器，分类器的输入是一个二维特征向量 `[我早上是否和她说了早安，我最近一周是否给她购买过礼物]`，分类器的输出是`开心/不开心`。训练样本如下：

训练样本 | 说早安 | 买礼物 | 女友心情 
------- | ----- | ----- | ------- 
样本1     |  是      | 是       | 开心        
样本2     |  否      | 是       | 开心        
样本3     |  是      | 否       | 不开心     
样本4     |  否      | 否       | 不开心     

分类器训练好了，我满心欢喜地来预测今天女友今天是否开心，`[说了早安，买了礼物]`，结果女友今天吐槽我说“我同事的老公又给她买了新的项链，我**不开心**”。显然，我们的分类器失效了。
事实上，根据谚语"女人的心情，三分天注定，七分靠shopping"，我们可以推断出女友心情应该是随机且不可预测的。LR模型在该场景完败，奉劝大家还是多买买买来讨好女朋友。

### 性质2的说明
针对性质2，一个样本只可能属于一个分类标签，这个比较好理解，LR模型的分类结果是样本i属于类别C，不存在样本i既属于类别C又属于类别D的情况。如果样本可能属于多个分类，例如薛定谔的猫，就不适合使用LR模型来分类。

### 总结
LR模型建立在 “样本分类标签满足二项分布”的假设上。因此，对于不满足二项分布的场景，LR模型无法准确分类。解决的途径可以是增加非线性特征、组合特征等，使分类标签在更高维特征空间满足二项分布；或是选择决策树、随机森林等非线性模型。
